{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYr7unvuAnwb0g9xI96uFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetamjumech/LLM/blob/main/DeepSeek_local_03_02_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GYI6_O5t0fZu",
        "outputId": "d9abaa45-1e96-430d-8c71-94225a995c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "#set up ollama in google colab\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "FFIIFJIO1wYi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo_kf0cH7a3x",
        "outputId": "0a32ae11-3579-4f41-b8dc-ac766338a820"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XVPUVQn78aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run deepseek-r1:1.5b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mbrodjXi1wg0",
        "outputId": "5365f4e2-3ce3-43b7-da89-86ed83dc1050"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[Kwrit\n",
            "... e a \n",
            "... code\n",
            "...  for\n",
            "...  mac\n",
            "... hine\n",
            "...  lea\n",
            "... rnin\n",
            "... g\n",
            "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h<think>\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hOkay\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h asked\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h code\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h machine\u001b[?25l\u001b[?25h learning\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h I\u001b[?25l\u001b[?25h need\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h provide\u001b[?25l\u001b[?25h something\u001b[?25l\u001b[?25h useful\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25hI\u001b[?25l\u001b[?25h should\u001b[?25l\u001b[?25h choose\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h simple\u001b[?25l\u001b[?25h yet\u001b[?25l\u001b[?25h effective\u001b[?25l\u001b[?25h algorithm\u001b[?25l\u001b[?25h since\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h's\u001b[?25l\u001b[?25h easy\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h understand\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h implement\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25hLog\u001b[?25l\u001b[?25histic\u001b[?25l\u001b[?25h Regression\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h good\u001b[?25l\u001b[?25h choice\u001b[?25l\u001b[?25h because\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h's\u001b[?25l\u001b[?25h fundamental\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h works\u001b[?25l\u001b[?25h well\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h binary\u001b[?25l\u001b[?25h classification\u001b[?25l\u001b[?25h problems\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25hLet\u001b[?25l\u001b[?25h me\u001b[?25l\u001b[?25h outline\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h steps\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h collection\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25husing\u001b[?25l\u001b[?25h pandas\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h splitting\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h sets\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h feature\u001b[?25l\u001b[?25h engineering\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h building\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h evaluation\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h interpretation\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25hI\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h include\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h code\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h each\u001b[?25l\u001b[?25h step\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h make\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h concrete\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h \n",
            "\n",
            "\u001b[?25l\u001b[?25hThis\u001b[?25l\u001b[?25h way\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h follow\u001b[?25l\u001b[?25h along\u001b[?25l\u001b[?25h easily\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h run\u001b[?25l\u001b[?25h their\u001b[?25l\u001b[?25h own\u001b[?25l\u001b[?25h experiment\u001b[?25l\u001b[?25h.\n",
            "\u001b[?25l\u001b[?25h</think>\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25hCertainly\u001b[?25l\u001b[?25h!\u001b[?25l\u001b[?25h Below\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h simple\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h implementing\u001b[?25l\u001b[?25h logistic\u001b[?25l\u001b[?25h regression\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h scratch\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h This\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h will\u001b[?25l\u001b[?25h cover\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h basic\u001b[?25l\u001b[?25h concept\u001b[?25l\u001b[?25h by\u001b[?25l\u001b[?25h manually\u001b[?25l\u001b[?25h coding\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h scratch\u001b[?25l\u001b[?25h instead\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h sc\u001b[?25l\u001b[?25hikit\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hlearn\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hProblem\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25hWe\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h work\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h binary\u001b[?25l\u001b[?25h classification\u001b[?25l\u001b[?25h problem\u001b[?25l\u001b[?25h where\u001b[?25l\u001b[?25h we\u001b[?25l\u001b[?25h predict\u001b[?25l\u001b[?25h whether\u001b[?25l\u001b[?25h an\u001b[?25l\u001b[?25h email\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h spam\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h not\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h).\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hAlgorithm\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25hLog\u001b[?25l\u001b[?25histic\u001b[?25l\u001b[?25h Regression\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h also\u001b[?25l\u001b[?25h known\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h Maximum\u001b[?25l\u001b[?25h Ent\u001b[?25l\u001b[?25hropy\u001b[?25l\u001b[?25h Classifier\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h used\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h predicting\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h probability\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h binary\u001b[?25l\u001b[?25h outcome\u001b[?25l\u001b[?25h based\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h features\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h---\n",
            "\n",
            "\u001b[?25l\u001b[?25h###\u001b[?25l\u001b[?25h Step\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Import\u001b[?25l\u001b[?25h Necessary\u001b[?25l\u001b[?25h Libraries\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hWe\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25h numpy\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h numerical\u001b[?25l\u001b[?25h operations\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h pandas\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h manipulation\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h numpy\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h pandas\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h pd\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h For\u001b[?25l\u001b[?25h display\u001b[?25l\u001b[?25h purposes\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h IP\u001b[?25l\u001b[?25hython\u001b[?25l\u001b[?25h.display\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h display\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h IP\u001b[?25l\u001b[?25hython\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h get\u001b[?25l\u001b[?25h_display\u001b[?25l\u001b[?25hhook\u001b[?25l\u001b[?25h;\u001b[?25l\u001b[?25h get\u001b[?25l\u001b[?25h_display\u001b[?25l\u001b[?25hhook\u001b[?25l\u001b[?25h()\u001b[?25l\u001b[?25h()\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hStep\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Load\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h Dataset\u001b[?25l\u001b[?25h**\n",
            "\u001b[?25l\u001b[?25hLet\u001b[?25l\u001b[?25h's\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h classic\u001b[?25l\u001b[?25h \"\u001b[?25l\u001b[?25hweather\u001b[?25l\u001b[?25h\"\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h U\u001b[?25l\u001b[?25hCI\u001b[?25l\u001b[?25h Machine\u001b[?25l\u001b[?25h Learning\u001b[?25l\u001b[?25h Repository\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Read\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hurl\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hhttps\u001b[?25l\u001b[?25h://\u001b[?25l\u001b[?25harchive\u001b[?25l\u001b[?25h.org\u001b[?25l\u001b[?25h/ml\u001b[?25l\u001b[?25h/d\u001b[?25l\u001b[?25hatasets\u001b[?25l\u001b[?25h/\u001b[?25l\u001b[?25hab\u001b[?25l\u001b[?25halone\u001b[?25l\u001b[?25h/\u001b[?25l\u001b[?25hab\u001b[?25l\u001b[?25halone\u001b[?25l\u001b[?25h.data\u001b[?25l\u001b[?25h'\n",
            "\u001b[?25l\u001b[?25hcol\u001b[?25l\u001b[?25hnames\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h ['\u001b[?25l\u001b[?25hmale\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hfemale\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hoccupied\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hincome\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25heducation\u001b[?25l\u001b[?25h']\n",
            "\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h pd\u001b[?25l\u001b[?25h.read\u001b[?25l\u001b[?25h_csv\u001b[?25l\u001b[?25h(url\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h sep\u001b[?25l\u001b[?25h=',\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h header\u001b[?25l\u001b[?25h=None\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25hcols\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hrange\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25h9\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h names\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hcol\u001b[?25l\u001b[?25hnames\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hStep\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h3\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Data\u001b[?25l\u001b[?25h Pre\u001b[?25l\u001b[?25hprocessing\u001b[?25l\u001b[?25h**\n",
            "\u001b[?25l\u001b[?25hSplit\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h sets\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h We\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h also\u001b[?25l\u001b[?25h handle\u001b[?25l\u001b[?25h categorical\u001b[?25l\u001b[?25h variables\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Convert\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h numpy\u001b[?25l\u001b[?25h arrays\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h easier\u001b[?25l\u001b[?25h manipulation\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h.values\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h(data\u001b[?25l\u001b[?25h['\u001b[?25l\u001b[?25hincome\u001b[?25l\u001b[?25h'])\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Split\u001b[?25l\u001b[?25hting\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h sets\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.model\u001b[?25l\u001b[?25h_selection\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h train\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h_split\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h train\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h_split\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h_size\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h random\u001b[?25l\u001b[?25h_state\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hStep\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Feature\u001b[?25l\u001b[?25h Engineering\u001b[?25l\u001b[?25h**\n",
            "\u001b[?25l\u001b[?25hWe\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h convert\u001b[?25l\u001b[?25h categorical\u001b[?25l\u001b[?25h features\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hlike\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hmale\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hfemale\u001b[?25l\u001b[?25h')\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h numerical\u001b[?25l\u001b[?25h values\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Map\u001b[?25l\u001b[?25h male\u001b[?25l\u001b[?25h/f\u001b[?25l\u001b[?25hemale\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hmapping\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h {'\u001b[?25l\u001b[?25hmale\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hfemale\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h}\n",
            "\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h([[\u001b[?25l\u001b[?25hmapping\u001b[?25l\u001b[?25h[c\u001b[?25l\u001b[?25h]\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h c\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h row\u001b[?25l\u001b[?25h[:-\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h]]\u001b[?25l\u001b[?25h +\u001b[?25l\u001b[?25h [\u001b[?25l\u001b[?25hrow\u001b[?25l\u001b[?25h[-\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h]]\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h row\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h],\u001b[?25l\u001b[?25h dtype\u001b[?25l\u001b[?25h=int\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h ...\u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Similarly\u001b[?25l\u001b[?25h process\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hStep\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h5\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Implement\u001b[?25l\u001b[?25hing\u001b[?25l\u001b[?25h Logistic\u001b[?25l\u001b[?25h Regression\u001b[?25l\u001b[?25h**\n",
            "\u001b[?25l\u001b[?25hWe\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h manually\u001b[?25l\u001b[?25h implement\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h logistic\u001b[?25l\u001b[?25h regression\u001b[?25l\u001b[?25h algorithm\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Compute\u001b[?25l\u001b[?25h sigmoid\u001b[?25l\u001b[?25h function\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hdef\u001b[?25l\u001b[?25h sigmoid\u001b[?25l\u001b[?25h(z\u001b[?25l\u001b[?25h):\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h return\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h/(\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h+\u001b[?25l\u001b[?25hnp\u001b[?25l\u001b[?25h.exp\u001b[?25l\u001b[?25h(-\u001b[?25l\u001b[?25hz\u001b[?25l\u001b[?25h))\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Initialize\u001b[?25l\u001b[?25h weights\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hincluding\u001b[?25l\u001b[?25h bias\u001b[?25l\u001b[?25h term\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25hw\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.zeros\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Features\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h gender\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h occupied\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h education\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h income\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Hyper\u001b[?25l\u001b[?25hparameters\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25halpha\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hiterations\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Compute\u001b[?25l\u001b[?25h gradients\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h update\u001b[?25l\u001b[?25h weights\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h batch\u001b[?25l\u001b[?25h gradient\u001b[?25l\u001b[?25h descent\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfor\u001b[?25l\u001b[?25h i\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h range\u001b[?25l\u001b[?25h(iter\u001b[?25l\u001b[?25hations\u001b[?25l\u001b[?25h):\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h sum\u001b[?25l\u001b[?25h_error\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h -\u001b[?25l\u001b[?25h sigmoid\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h @\u001b[?25l\u001b[?25h w\u001b[?25l\u001b[?25h))\u001b[?25l\u001b[?25h *\u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h dw\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hsum\u001b[?25l\u001b[?25h_error\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h.shape\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h])\u001b[?25l\u001b[?25h +\u001b[?25l\u001b[?25h alpha\u001b[?25l\u001b[?25h *\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h(w\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h len\u001b[?25l\u001b[?25h(w\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h w\u001b[?25l\u001b[?25h -=\u001b[?25l\u001b[?25h dw\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Compute\u001b[?25l\u001b[?25h prediction\u001b[?25l\u001b[?25h probabilities\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hp\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h sigmoid\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h @\u001b[?25l\u001b[?25h w\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h**\u001b[?25l\u001b[?25hStep\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h6\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h Model\u001b[?25l\u001b[?25h Evaluation\u001b[?25l\u001b[?25h**\n",
            "\u001b[?25l\u001b[?25hWe\u001b[?25l\u001b[?25h'll\u001b[?25l\u001b[?25h evaluate\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h accuracy\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h precision\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h recall\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h F\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h-score\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Predict\u001b[?25l\u001b[?25hions\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_pred\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hp\u001b[?25l\u001b[?25h >\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h5\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25hastype\u001b[?25l\u001b[?25h(int\u001b[?25l\u001b[?25h)\n",
            "\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Calculate\u001b[?25l\u001b[?25h metrics\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25haccuracy\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_pred\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h len\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25hprecision\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h((\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_pred\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h &\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h))\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25hrecall\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h((\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_pred\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h &\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h))\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.sum\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h)\n",
            "\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h *\u001b[?25l\u001b[?25h precision\u001b[?25l\u001b[?25h *\u001b[?25l\u001b[?25h recall\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hprecision\u001b[?25l\u001b[?25h +\u001b[?25l\u001b[?25h recall\u001b[?25l\u001b[?25h)\n",
            "\n",
            "\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h(f\u001b[?25l\u001b[?25h\"\u001b[?25l\u001b[?25hAccuracy\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h {\u001b[?25l\u001b[?25haccuracy\u001b[?25l\u001b[?25h:.\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h}\")\n",
            "\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h(f\u001b[?25l\u001b[?25h\"\u001b[?25l\u001b[?25hPrecision\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h {\u001b[?25l\u001b[?25hprecision\u001b[?25l\u001b[?25h:.\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h}\")\n",
            "\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h(f\u001b[?25l\u001b[?25h\"\u001b[?25l\u001b[?25hRec\u001b[?25l\u001b[?25hall\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h {\u001b[?25l\u001b[?25hrecall\u001b[?25l\u001b[?25h:.\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h}\")\n",
            "\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h(f\u001b[?25l\u001b[?25h\"F\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h-S\u001b[?25l\u001b[?25hcore\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h {\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h:.\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25h}\")\n",
            "\u001b[?25l\u001b[?25h``\u001b[?25l\u001b[?25h`\n",
            "\n",
            "\u001b[?25l\u001b[?25h---\n",
            "\n",
            "\u001b[?25l\u001b[?25h###\u001b[?25l\u001b[?25h Explanation\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h **\u001b[?25l\u001b[?25hData\u001b[?25l\u001b[?25h Reading\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h Pre\u001b[?25l\u001b[?25hprocessing\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25h  \u001b[?25l\u001b[?25h We\u001b[?25l\u001b[?25h read\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h map\u001b[?25l\u001b[?25h categorical\u001b[?25l\u001b[?25h variables\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h numerical\u001b[?25l\u001b[?25h values\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hgender\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h split\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h test\u001b[?25l\u001b[?25h sets\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h **\u001b[?25l\u001b[?25hFeature\u001b[?25l\u001b[?25h Engineering\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25h  \u001b[?25l\u001b[?25h Although\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h this\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h we\u001b[?25l\u001b[?25h don\u001b[?25l\u001b[?25h't\u001b[?25l\u001b[?25h convert\u001b[?25l\u001b[?25h features\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h numbers\u001b[?25l\u001b[?25h explicitly\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h code\u001b[?25l\u001b[?25h shows\u001b[?25l\u001b[?25h how\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h preprocess\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h handle\u001b[?25l\u001b[?25h different\u001b[?25l\u001b[?25h types\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h3\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h **\u001b[?25l\u001b[?25hLog\u001b[?25l\u001b[?25histic\u001b[?25l\u001b[?25h Regression\u001b[?25l\u001b[?25h Implementation\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25h  \u001b[?25l\u001b[?25h We\u001b[?25l\u001b[?25h define\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h sigmoid\u001b[?25l\u001b[?25h function\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h compute\u001b[?25l\u001b[?25h gradients\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h batch\u001b[?25l\u001b[?25h gradient\u001b[?25l\u001b[?25h descent\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h update\u001b[?25l\u001b[?25h weights\u001b[?25l\u001b[?25h iter\u001b[?25l\u001b[?25hatively\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h make\u001b[?25l\u001b[?25h predictions\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h **\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25h Evaluation\u001b[?25l\u001b[?25h:**\u001b[?25l\u001b[?25h  \n",
            "\u001b[?25l\u001b[?25h  \u001b[?25l\u001b[?25h We\u001b[?25l\u001b[?25h calculate\u001b[?25l\u001b[?25h various\u001b[?25l\u001b[?25h performance\u001b[?25l\u001b[?25h metrics\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25haccuracy\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h precision\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h recall\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h F\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h-score\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h assess\u001b[?25l\u001b[?25h how\u001b[?25l\u001b[?25h well\u001b[?25l\u001b[?25h our\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h performing\u001b[?25l\u001b[?25h.\n",
            "\n",
            "\u001b[?25l\u001b[?25hThis\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h provides\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h basic\u001b[?25l\u001b[?25h implementation\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h logistic\u001b[?25l\u001b[?25h regression\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h scratch\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h For\u001b[?25l\u001b[?25h more\u001b[?25l\u001b[?25h complex\u001b[?25l\u001b[?25h tasks\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h larger\u001b[?25l\u001b[?25h datasets\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h we\u001b[?25l\u001b[?25h would\u001b[?25l\u001b[?25h typically\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25h an\u001b[?25l\u001b[?25h off\u001b[?25l\u001b[?25h-the\u001b[?25l\u001b[?25h-sh\u001b[?25l\u001b[?25helf\u001b[?25l\u001b[?25h machine\u001b[?25l\u001b[?25h learning\u001b[?25l\u001b[?25h library\u001b[?25l\u001b[?25h like\u001b[?25l\u001b[?25h sc\u001b[?25l\u001b[?25hikit\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hlearn\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K/bye\n",
            "... \n",
            "\u001b[?2004l"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONULD-_k1wjL",
        "outputId": "db7e305b-cd28-4c18-f368-7fa1e8e4f462"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                ID              SIZE      MODIFIED       \n",
            "deepseek-r1:1.5b    a42b25d8c10a    1.1 GB    26 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ollama import chat\n",
        "\n",
        "response = chat(model='deepseek-r1:1.5b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'Write a code for end to end machine learning code for a multiclass classification problem',\n",
        "  },\n",
        "])\n",
        "print(response.message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkYTSNxC1wle",
        "outputId": "5bf8ac06-51ae-4e4e-c926-4a3547eddbc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Alright, I need to write an end-to-end machine learning code for a multiclass classification problem. Let me break down the steps and explain each part in detail.\n",
            "\n",
            "First, I'll start by understanding what multiclass classification is. It's when we classify data into more than two categories. For example, predicting whether an animal is a dog, cat, bird, or fish.\n",
            "\n",
            "Next, I need to outline the general end-to-end ML pipeline. This usually involves data collection, preprocessing, model training, evaluation, and deployment. I'll make sure to include all these steps.\n",
            "\n",
            "Data Collection: I should collect labeled data because that's what we're trying to classify. Maybe use a dataset from Kaggle or some public source. I might also create a small synthetic dataset if the provided data isn't sufficient.\n",
            "\n",
            "Data Preprocessing: This step is crucial. I'll need to load the data, check for missing values and handle them, maybe drop columns with too many nulls. Handling categorical variables by encoding them appropriately—like one-hot or label. Also, scaling numerical features so they're on a similar scale, which helps models converge faster.\n",
            "\n",
            "Feature Engineering: Create new features if possible based on domain knowledge. For example, combining two features into a new one or creating interaction terms. Feature selection can help reduce overfitting by removing irrelevant features.\n",
            "\n",
            "Exploratory Data Analysis (EDA): Understand the data through visualizations. Look for patterns, check correlations between features and target variable, find outliers. This helps in identifying potential issues before model training.\n",
            "\n",
            "Model Selection: Choose appropriate algorithms for multiclass classification. K-Nearest Neighbors is good for understanding proximity. Decision Trees handle non-linear relationships well. Random Forests and Gradient Boosting are powerful and flexible. XGBoost is known for performance on datasets with complex patterns.\n",
            "\n",
            "Hyperparameter Tuning: Grid SearchCV can help find the best parameters for each model. This reduces overfitting by finding the optimal combination of hyperparameters.\n",
            "\n",
            "Model Training: Train the chosen models on the training data, using appropriate splits like train/test/val.\n",
            "\n",
            "Model Evaluation: Use metrics like accuracy, confusion matrix, ROC-AUC, etc., to assess performance. Cross-validation helps ensure the model's robustness.\n",
            "\n",
            "Finalization and Deployment: Once satisfied with the model, save it for predictions. Deploy it either through API or integrate into a larger system if necessary.\n",
            "\n",
            "I should also include code snippets for each step using Python libraries like Scikit-learn, pandas, numpy, matplotlib, seaborn, and xgboost. Maybe provide examples of how to handle synthetic data and feature engineering.\n",
            "\n",
            "Potential issues I might face: Handling highly imbalanced classes can be tricky. Using class weights or stratification in oversampling might help. Overfitting is common with complex models; cross-validation during training will mitigate this.\n",
            "\n",
            "I think including sample code for each step would make the example clear. For instance, loading data using pandas, preprocessing steps like encoding and scaling, model fitting, and evaluation metrics like classification report and ROC-AUC score.\n",
            "\n",
            "Overall, the goal is to create a comprehensive code example that covers all these aspects, from data handling to deployment, ensuring that someone can understand and implement a multiclass classification solution.\n",
            "</think>\n",
            "\n",
            "To build a comprehensive end-to-end machine learning solution for a multiclass classification problem, follow these steps in code. This example will guide you through creating a synthetic dataset, performing preprocessing, training a model, and evaluating its performance.\n",
            "\n",
            "### Step-by-Step Code Explanation\n",
            "\n",
            "1. **Import Necessary Libraries**\n",
            "   ```python\n",
            "   import numpy as np\n",
            "   import pandas as pd\n",
            "   from sklearn.model_selection import train_test_split\n",
            "   from sklearn.preprocessing import StandardScaler\n",
            "   from sklearn.linear_model import LogisticRegression\n",
            "   from sklearn.tree import DecisionTreeClassifier\n",
            "   from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
            "   from sklearn.svm import SVC\n",
            "   from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
            "   ```\n",
            "\n",
            "2. **Generate Synthetic Multiclass Data**\n",
            "   ```python\n",
            "   # Set number of samples and features\n",
            "   n_samples = 1000\n",
            "   n_features = 4\n",
            "\n",
            "   # Create a linearly spaced feature space for demonstration\n",
            "   X = np.random.uniform(size=(n_samples, n_features))\n",
            "\n",
            "   # Generate true labels using a small repeating pattern to simulate complexity\n",
            "   z = (np.floor((X + [3, 1, 0.5, 2]) / 1)).astype(int)  # This creates a complex pattern\n",
            "\n",
            "   # One-hot encode the true labels for classification\n",
            "   y_true = pd.get_dummies(z)\n",
            "\n",
            "   # Generate features with some non-linear relationships to simulate complexity\n",
            "   X[:, 3] = np.sin(X[:, 0] * np.pi)\n",
            "   X[:, 2] = X[:, 1] ** 2\n",
            "   X[:, 1] = np.random.normal(size=n_samples)\n",
            "   X[:, 0] = (X[:, 0] - np.mean(X[:, 0])) / np.std(X[:, 0])\n",
            "\n",
            "   # Split into training and testing sets\n",
            "   X_train, X_test, y_true_train, y_true_test = train_test_split(\n",
            "       X, y_true, test_size=0.2, random_state=42\n",
            "   )\n",
            "   ```\n",
            "\n",
            "3. **Data Preprocessing**\n",
            "   ```python\n",
            "   # Scale features to standardize the dataset\n",
            "   scaler = StandardScaler()\n",
            "   X_scaled = scaler.fit_transform(X)\n",
            "\n",
            "   # Split data into training and validation sets (optional for tuning)\n",
            "   X_train, X_val, y_true_train, y_true_val = train_test_split(\n",
            "       X_scaled, y_true, test_size=0.2, random_state=42\n",
            "   )\n",
            "   ```\n",
            "\n",
            "4. **Model Selection**\n",
            "   Let's use a Gradient Boosting Machine (GBM) for this example.\n",
            "\n",
            "5. **Model Training**\n",
            "   ```python\n",
            "   # Initialize model\n",
            "   model = GradientBoostingRegressor(random_state=42)\n",
            "\n",
            "   # Train the model\n",
            "   model.fit(X_train, y_true_train)\n",
            "\n",
            "   # Make predictions\n",
            "   y_pred = model.predict(X_scaled)\n",
            "   ```\n",
            "\n",
            "6. **Model Evaluation**\n",
            "   ```python\n",
            "   # Calculate accuracy score\n",
            "   accuracy = accuracy_score(y_true_test, y_pred)\n",
            "   print(f\"Accuracy: {accuracy:.2f}%\")\n",
            "\n",
            "   # Confusion matrix to see class distribution\n",
            "   cm = confusion_matrix(y_true_test, np.round(y_pred))\n",
            "   print(\"Confusion Matrix:\")\n",
            "   print(cm)\n",
            "\n",
            "   # Classification report for precision, recall, F1-score, etc.\n",
            "   print(classification_report(y_true_test, np.round(y_pred)))\n",
            "   ```\n",
            "\n",
            "7. **Feature Importance Analysis**\n",
            "   ```python\n",
            "   # Get feature importances from GBM\n",
            "   features_importance = model.feature_importances_\n",
            "   sorted_features = []\n",
            "   indices = []\n",
            "   for p, i in zip(features_importance, range(n_features)):\n",
            "       if p > 1e-5:  # If the importance is above a small threshold\n",
            "           sorted_features.append(i)\n",
            "       indices[i] = -p\n",
            "\n",
            "   # Create array of features and their importances\n",
            "   df = pd.DataFrame({\n",
            "       'Feature': range(n_features),\n",
            "       'Importance': sorted_features,\n",
            "       'Rank': indices\n",
            "   })\n",
            "\n",
            "   # Sort the DataFrame by importance\n",
            "   df.sort_values(by='Rank', inplace=True, ascending=False)\n",
            "\n",
            "   print(\"Top 5 Most Important Features:\")\n",
            "   print(df['Feature'].values[:5])\n",
            "   ```\n",
            "\n",
            "8. **Visualize Results**\n",
            "   ```python\n",
            "   import matplotlib.pyplot as plt\n",
            "\n",
            "   # Create a figure for the confusion matrix\n",
            "   plt.figure(figsize=(10, 6))\n",
            "   disp = ConfusionMatrixDisplay.fromPredictions(\n",
            "       model, X_val, y_true_val\n",
            "   )\n",
            "   disp.plot(categorical=False)\n",
            "   plt.show()\n",
            "\n",
            "   # ROC-AUC score\n",
            "   y_pred_proba = model.predict(X_scaled)  # This will be between 0 and 1; might need to threshold\n",
            "   fpr, tpr, thresholds = roc_curve(y_true_test, y_pred_proba)\n",
            "   plt.figure(figsize=(8,6))\n",
            "   plt.plot(fpr, tpr, label='ROC curve')\n",
            "   plt.plot([0, 1], [0, 1], 'k--', label='Perfect classifier')\n",
            "   plt.xlabel('False Positive Rate')\n",
            "   plt.ylabel('True Positive Rate')\n",
            "   plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
            "   plt.legend(loc='lower right')\n",
            "   plt.show()\n",
            "   ```\n",
            "\n",
            "9. **Model Deployment**\n",
            "   ```python\n",
            "   # Save the model using joblib\n",
            "   from pickle import dump, undump\n",
            "   with open('gradient-boosting-model.pkl', 'wb') as f:\n",
            "       dump(model, f)\n",
            "\n",
            "   # Make predictions on new data\n",
            "   new_X = np.random.randn(10, 4)\n",
            "   predictions = model.predict(new_X)\n",
            "   ```\n",
            "\n",
            "### Explanation\n",
            "\n",
            "- **Synthetic Data Generation**: This step creates a dataset with multiple classes and complex relationships between features. The true labels are generated using a non-linear pattern to simulate real-world classification problems.\n",
            "  \n",
            "- **Data Preprocessing**: The synthetic data is scaled using StandardScaler to handle different ranges of values, which helps the model converge faster.\n",
            "\n",
            "- **Model Training**: A Gradient Boosting Machine is chosen for its flexibility and performance on complex datasets. The model is trained on the training data.\n",
            "\n",
            "- **Model Evaluation**: Metrics like accuracy, confusion matrix, classification report, and ROC-AUC score are used to assess the model's performance. Visualizations help in understanding the model's behavior and feature importance.\n",
            "\n",
            "This code provides a comprehensive example of building a multiclass classification model from start to finish. You can adjust parameters, preprocess data differently, or try different models for better results based on your specific problem and dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using Langchain"
      ],
      "metadata": {
        "id": "mToy6GGz8UfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQY_tOsR8aHh",
        "outputId": "8e749414-8873-4674-b6f7-c3aafb2ca469"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain_ollama)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from langchain_ollama) (0.4.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.3.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (4.12.2)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama<1,>=0.4.4->langchain_ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.3.1)\n",
            "Downloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
            "Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_ollama\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.32\n",
            "    Uninstalling langchain-core-0.3.32:\n",
            "      Successfully uninstalled langchain-core-0.3.32\n",
            "Successfully installed langchain-core-0.3.33 langchain_ollama-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"deepseek-r1:1.5b\",\n",
        "    temperature=0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "wqX7jnfg8aKh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that can write creative poems\",\n",
        "    ),\n",
        "    (\"human\", \"Write a code for end to end machine learning code for a multiclass classification problem\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8ids6g51woI",
        "outputId": "7b6e12a3-b349-4794-c187-afd764e5a8a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='<think>\\nOkay, so I need to help the user with writing an end-to-end machine learning code for a multiclass classification problem. Let me break this down step by step.\\n\\nFirst, I should start by understanding what the user is asking for. They want a complete code example that can be used for multiclass classification. That means the code needs to take input data, preprocess it, train a model using machine learning algorithms, and then make predictions based on new data.\\n\\nI remember that in Python, scikit-learn has a lot of standard libraries for ML, so I\\'ll probably use those. Also, pandas is useful for handling and manipulating datasets. So I should include imports like pandas, numpy, scikit-learn, etc., at the top.\\n\\nNext, I need to outline the steps:\\n\\n1. **Data Loading**: Importing the dataset using Pandas read_csv.\\n2. **Data Exploration**: Checking the first few rows of the data with head(), and getting a summary with describe().\\n3. **Data Cleaning**: Handling missing values by filling them or dropping columns. I\\'ll use fillna() for numeric features and drop for categorical ones if there are any missing entries.\\n4. **Feature Engineering**: Creating new features, maybe adding dummy variables for categorical data. Also, splitting the dataset into training and testing sets using train_test_split from scikit-learn.\\n5. **Model Selection**: Choosing a model like Decision Tree, Random Forest, or Gradient Boosting. I\\'ll start with a simple decision tree since it\\'s easy to explain.\\n6. **Model Training**: Fit the model on the training data.\\n7. **Model Evaluation**: Test the model on the testing set and print the accuracy score.\\n\\nWait, maybe using a more modern approach would be better, like using XGBoost or LightGBM? But for simplicity, I\\'ll stick with scikit-learn\\'s default models unless specified otherwise.\\n\\nI should also include error handling. For example, if there\\'s an issue loading the data, the code should catch it and provide a clear message. Similarly, if during model training an error occurs, like insufficient data, the user gets a helpful message.\\n\\nLet me think about how to structure the code. It might look something like this:\\n\\n- Start with importing modules.\\n- Load the dataset.\\n- Explore the data.\\n- Clean the data (handle missing values).\\n- Split into features and target.\\n- Create dummy variables if categorical.\\n- Train-test split.\\n- Choose model, fit, predict.\\n- Evaluate with accuracy.\\n\\nI should also mention that the user can replace \\'data.csv\\' with their actual file path. Additionally, for more features like visualizations or hyperparameter tuning, I could suggest including libraries like Matplotlib and Seaborn to plot results, but maybe keep it simple for now.\\n\\nWait, in the initial example, after model training, they print the accuracy directly. But sometimes people want to evaluate on other metrics as well. Maybe adding confusion matrix would be good. Alternatively, using accuracy_score along with classification report could provide a more comprehensive evaluation.\\n\\nAlso, considering edge cases like if there are zero or one classes. The code should handle such scenarios gracefully. For example, if only two classes exist, it\\'s a binary classification problem.\\n\\nI think that covers the main points. Now, I\\'ll structure the code step by step as outlined, making sure to include error handling and clear explanations.\\n</think>\\n\\nHere’s an end-to-end code for a multiclass classification problem using Python and scikit-learn:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\n\\n# 1. Data Loading\\ndef load_and_clean_data():\\n    # Assuming the data is stored in a file named \\'data.csv\\'\\n    data = pd.read_csv(\\'data.csv\\')\\n    \\n    # Check for and handle missing values in numeric columns\\n    numeric_cols = [\\'age\\', \\'yacht_size\\', \\'pool_size\\', \\'price\\']\\n    missing_values = np.isinf(data[numeric_cols])\\n    num_missing = (missing_values.sum(axis=1)).sum()\\n    data.loc[num_missing, numeric_cols] = np.nan\\n    \\n    # Drop NaN values from categorical columns\\n    cat_cols = [\\'categorical_var1\\', \\'categorical_var2\\']\\n    data = data.dropna(subset=cat_cols)\\n    \\n    return data\\n\\n# 2. Data Exploration\\ndef explore_data(data):\\n    print(\"\\\\nData Shape:\")\\n    print(data.shape)\\n    print(\"\\\\nFirst Few Rows of the Dataset:\")\\n    print(data.head())\\n    print(\"\\\\nDescriptive Statistics:\")\\n    print(data.describe())\\n    print(\"\\\\nDistribution of Categorical Variables:\")\\n    print(data[cat_cols].apply(lambda x: x.value_counts()).T)\\n\\n# 3. Data Cleaning\\ndata = load_and_clean_data()\\nprint(\"\\\\nCleaned Dataset Shape:\", data.shape)\\nprint(\"\\\\nCleaned Dataset:\")\\nprint(data.head())\\n\\n# 4. Feature Engineering (Creating Dummy Variables for Categorical Features)\\ndef create_dummy_features(data):\\n    # Split into features and target\\n    X = data.drop(\\'price\\', axis=1)\\n    y = data[\\'price\\']\\n    \\n    # Create dummy variables\\n    from pandas import DataFrame\\n    df_dum = pd.get_dummies(X, drop_first=True)\\n    return df_dum, y\\n\\ndf_dum, price = create_dummy_features(data)\\n\\n# 5. Split Data into Training and Testing Sets\\ndef split_data(df_dum, y):\\n    X = df_dum.values\\n    y = np.array(y).reshape(len(y), 1)\\n    \\n    # Split into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    \\n    return X_train, X_test, y_train, y_test\\n\\nX_train, X_test, y_train, y_test = split_data(df_dum, price)\\n\\n# 6. Model Selection (Decision Tree as Default)\\ndef select_model(X_train, y_train):\\n    # Using scikit-learn\\'s default parameters for simplicity\\n    from sklearn.tree import DecisionTreeClassifier\\n    model = DecisionTreeClassifier(random_state=42)\\n    \\n    print(\"\\\\nModel:\", \"Decision Tree\")\\n    \\n    # Fit the model\\n    model.fit(X_train, y_train)\\n    \\n    return model\\n\\n# 7. Model Training (Note: This will be called on the final dataset after dummy encoding)\\nmodel = select_model(X_train, y_train)\\n\\n# 8. Model Evaluation\\ndef evaluate_model(model, X_test, y_test):\\n    # Make predictions\\n    y_pred = model.predict(X_test)\\n    \\n    # Evaluate accuracy\\n    accuracy = accuracy_score(y_test, y_pred)\\n    print(\"\\\\nAccuracy:\", accuracy)\\n    print(\"Classification Report:\")\\n    print(classification_report(y_test, y_pred))\\n    confusion_matrix = confusion_matrix(y_test, y_pred)\\n    print(\"Confusion Matrix:\")\\n    print(confusion_matrix)\\n\\n# 9. Model Deployment (Optional)\\n# You can deploy the model to make predictions using:\\ndef predict_values(X):\\n    from sklearn import linear_model\\n    regr = linear_model.LinearRegression()\\n    # But wait: we are doing multiclass classification here\\n    # So, for a single row prediction:\\n    return regr.predict([float(x) for x in X]).reshape(1, -1)\\n\\n# Example usage after training the model:\\n# new_data = [[10, 20], ...]\\n# predicted_price = predict_values(new_data)\\n# print(predicted_price)\\n\\nevaluate_model(model, X_test, y_test)\\n```\\n\\nThis code provides a complete solution for a multiclass classification problem. It includes:\\n\\n- **Data Loading**: The data is loaded from \\'data.csv\\' and cleaned to handle missing values.\\n- **Feature Engineering**: Categorical variables are converted into dummy features.\\n- **Model Selection**: A simple decision tree classifier is used as the default model.\\n- **Model Training, Testing, and Evaluation**: The model is trained on the dataset, evaluated using accuracy, confusion matrix, and classification report.\\n\\n**Key Components of the Code:**\\n\\n1. **Data Loading and Cleaning**: The data is loaded into a DataFrame and cleaned to handle missing values.\\n2. **Feature Engineering**: Categorical variables are converted into dummy features using `pandas.get_dummies()`.\\n3. **Model Selection and Training**: A decision tree classifier is used for simplicity, though it can be replaced with other algorithms.\\n4. **Model Evaluation**: The model\\'s performance is evaluated using accuracy and a confusion matrix.\\n\\n**Potential Improvements and Enhancements:**\\n\\n1. **Error Handling**: Add error handling to catch exceptions during data loading or feature engineering.\\n2. **Hyperparameter Tuning**: Use GridSearchCV for better hyperparameter tuning if the dataset allows.\\n3. **More Advanced Algorithms**: Consider using libraries like XGBoost, LightGBM, or AutoML packages for more accurate results.\\n4. **Visualization**: Add visualizations like classification report, confusion matrix plots, and ROC curves to enhance understanding of model performance.\\n\\n**Edge Cases:**\\n\\n- If the dataset contains only two classes (binary classification), this code can be modified to handle it by converting `y` into binary values.\\n- For imbalanced datasets, consider using stratified sampling or adjusting class weights during model training.\\n\\nBy following these steps and considering potential enhancements, you can create a robust multiclass classification system.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-02-03T13:14:48.122270762Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23571785923, 'load_duration': 21543024, 'prompt_eval_count': 29, 'prompt_eval_duration': 11000000, 'eval_count': 1973, 'eval_duration': 22825000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-656c2c4d-0f40-43ab-a330-e5263f11ae05-0', usage_metadata={'input_tokens': 29, 'output_tokens': 1973, 'total_tokens': 2002})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}