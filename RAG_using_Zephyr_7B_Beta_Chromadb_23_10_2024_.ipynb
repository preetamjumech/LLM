{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOixJ00W5HmOnuTWijDN3Fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetamjumech/LLM/blob/main/RAG_using_Zephyr_7B_Beta_Chromadb_23_10_2024_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3egnFagNipd"
      },
      "outputs": [],
      "source": [
        "!pip install chainlit\n",
        "!pip install ctransformers\n",
        "!pip install torch\n",
        "!pip install sentence_transformers\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install PyPDF2\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain-text-splitters\n",
        "!pip install langchain_community\n",
        "!pip install langchain_core\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the model \"zephyr-7b-beta.Q5_K_S.gguf\""
      ],
      "metadata": {
        "id": "rsgSt_4aOJf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a folder called stores to store the embeddings in the current directory"
      ],
      "metadata": {
        "id": "RwDKHS1MOJij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store embeddings in your local directory, run python thisblockcode1.py\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "loader = PyPDFLoader(\"pet.pdf\")\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "vector_store = Chroma.from_documents(texts, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"stores/pet_cosine\")\n",
        "\n",
        "print(\"Vector Store Created.......\")"
      ],
      "metadata": {
        "id": "n3yTAm-VOJlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#python run thisblockcode2.py\n",
        "from langchain_core.prompts import PromptTemplate #LLMChain\n",
        "from langchain_community.llms import CTransformers\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from io import BytesIO\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "local_llm = \"zephyr-7b-beta.Q5_K_S.gguf\"\n",
        "\n",
        "config = {\n",
        "'max_new_tokens': 1024,\n",
        "'repetition_penalty': 1.1,\n",
        "'temperature': 0.1,\n",
        "'top_k': 50,\n",
        "'top_p': 0.9,\n",
        "'stream': True,\n",
        "'threads': int(os.cpu_count() / 2)\n",
        "}\n",
        "\n",
        "llm = CTransformers(\n",
        "    model=local_llm,\n",
        "    model_type=\"mistral\",\n",
        "    lib=\"avx2\", #for CPU use\n",
        "    **config\n",
        ")\n",
        "\n",
        "print(\"LLM Initialized...\")\n",
        "\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "load_vector_store = Chroma(persist_directory=\"stores/pet_cosine\", embedding_function=embeddings)\n",
        "retriever = load_vector_store.as_retriever(search_kwargs={\"k\":1})\n",
        "\n",
        "print(\"######################################################################\")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "\n",
        "sample_prompts = [\"what is the fastest speed for a greyhound dog?\", \"Why should we not feed chocolates to the dogs?\", \"Name two factors which might contribute to why some dogs might get scared?\"]\n",
        "\n",
        "def get_response(input):\n",
        "  query = input\n",
        "  chain_type_kwargs = {\"prompt\": prompt}\n",
        "  qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
        "  response = qa(query)\n",
        "  return response\n",
        "\n",
        "input = gr.Text(\n",
        "                label=\"Prompt\",\n",
        "                show_label=False,\n",
        "                max_lines=1,\n",
        "                placeholder=\"Enter your prompt\",\n",
        "                container=False,\n",
        "            )\n",
        "\n",
        "iface = gr.Interface(fn=get_response,\n",
        "             inputs=input,\n",
        "             outputs=\"text\",\n",
        "             title=\"My Dog PetCare Bot\",\n",
        "             description=\"This is a RAG implementation based on Zephyr 7B Beta LLM.\",\n",
        "             examples=sample_prompts,\n",
        "             allow_flagging=False\n",
        "             )\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "5nCY4durOxj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfidwg1KOsiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}